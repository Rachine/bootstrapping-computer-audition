# Macros:
# ==============================================================================
MUSDB_ROOT = '/home/data/musdb/raw/stems/'
SCAPER_PATH = '/home/data/musdb/formatted/generated/'
SCAPER_SOURCE_MATERIAL = '/home/data/music_symlinks/dpcl/'
TRAIN_CACHE = '/media/sdg/cache/musdb/tr'
TRAIN_FOLDER = '/home/data/musdb/formatted/generated/aug/coherent/train'
VAL_CACHE = '/media/sdg/cache/musdb/cv'
VAL_FOLDER = '/home/data/musdb/formatted/generated/aug/coherent/val'

# Parameters for output_folder:
# ==============================================================================
output_folder._output_folder = \
    '/home/pseetharaman/Dropbox/research/bootstrapping/musdb/exp/models/BootstrapYoutube/DPCL-Q2-Q5/'

# Parameters for train/fg/join_path:
# ==============================================================================
train/fg/join_path.base_path = %SCAPER_SOURCE_MATERIAL
train/fg/join_path.relative_path = 'above_q2'

# Parameters for Adam:
# ==============================================================================
Adam.lr = 0.001

# Parameters for add_auto_balance_loss:
# ==============================================================================
add_auto_balance_loss.update_frequency = 100

# Parameters for add_autoclip_gradient_handler:
# ==============================================================================
add_autoclip_gradient_handler.clip_percentile = 50

# Parameters for add_train_handlers:
# ==============================================================================
add_train_handlers.handler_names = \
    ['add_lr_scheduler_handler',
     'add_autoclip_gradient_handler',
     'add_auto_balance_loss']

# Parameters for analyze:
# ==============================================================================
analyze.output_folder = @output_folder()

# Parameters for BSSEvalScale:
# ==============================================================================
BSSEvalScale.compute_permutation = False
BSSEvalScale.source_labels = ['accompaniment', 'vocals']

# Parameters for test/build_dataset:
# ==============================================================================
test/build_dataset.dataset_class = @test/nussl.datasets.MixSourceFolder

# Parameters for train/build_dataset:
# ==============================================================================
train/build_dataset.dataset_class = @train/nussl.datasets.OnTheFly

# Parameters for val/build_dataset:
# ==============================================================================
val/build_dataset.dataset_class = @val/nussl.datasets.MixSourceFolder

# Parameters for build_model_optimizer_scheduler:
# ==============================================================================
build_model_optimizer_scheduler.model_config = \
    @build_chimera_with_mel()
build_model_optimizer_scheduler.optimizer_class = @torch.optim.Adam
build_model_optimizer_scheduler.scheduler_class = \
    @torch.optim.lr_scheduler.ReduceLROnPlateau
build_model_optimizer_scheduler.verbose = False

# Parameters for build_chimera_with_mel:
# ==============================================================================
build_chimera_with_mel.bidirectional = True
build_chimera_with_mel.dropout = 0.3
build_chimera_with_mel.embedding_activation = ['sigmoid', 'unit_norm']
build_chimera_with_mel.embedding_size = 20
build_chimera_with_mel.hidden_size = 300
build_chimera_with_mel.mask_activation = ['sigmoid']
build_chimera_with_mel.num_features = 257
build_chimera_with_mel.num_layers = 3
build_chimera_with_mel.num_sources = 2
build_chimera_with_mel.normalization_class = 'InstanceNorm'
build_chimera_with_mel.normalization_args = {'eps': 1e-10}
build_chimera_with_mel.num_mels = 300
build_chimera_with_mel.sample_rate = 16000

# Parameters for build_transforms:
# ==============================================================================
build_transforms.transform_names_and_args = \
    [('MagnitudeSpectrumApproximation', {}),
     ('MagnitudeWeights', {}),
     ('Cache', {}),
     ('ToSeparationModel', {})]

val/build_transforms.transform_names_and_args = \
    [('MagnitudeSpectrumApproximation', {}),
     ('Cache', {}),
     ('ToSeparationModel', {})]

# Parameters for train/build_transforms:
# ==============================================================================
train/build_transforms.cache_location = None

# Parameters for val/build_transforms:
# ==============================================================================
val/build_transforms.cache_location = None

# Parameters for cache:
# ==============================================================================
cache.batch_size = 1
cache.num_cache_workers = 1
cache.scopes = ['val']

# Parameters for train/create_mix_coherent_closure:
# ==============================================================================
train/create_mix_coherent_closure.bg_event_parameters = \
    {'label': ('choose', []),
     'source_file': ('choose', []),
     'source_time': ('uniform', 0, 1000)}
train/create_mix_coherent_closure.bg_path = @train/fg/join_path()
train/create_mix_coherent_closure.bitdepth = 16
train/create_mix_coherent_closure.fg_event_parameters = \
    {'event_duration': ('const', 12),
     'event_time': ('const', 0),
     'label': ('choose', ['vocals', 'accompaniment']),
     'pitch_shift': ('uniform', -2, 2),
     'snr': ('uniform', -10, 0),
     'source_file': ('choose', []),
     'source_time': ('uniform', 0, 1000),
     'time_stretch': ('uniform', 0.8, 1.2)}
train/create_mix_coherent_closure.fg_path = @train/fg/join_path()
train/create_mix_coherent_closure.ignore_warnings = True
train/create_mix_coherent_closure.labels = ['s0', 's1']
train/create_mix_coherent_closure.num_channels = 1
train/create_mix_coherent_closure.quick_pitch_time = False
train/create_mix_coherent_closure.ref_db = -40
train/create_mix_coherent_closure.sample_rate = 16000
train/create_mix_coherent_closure.scene_duration = 10
get_loud_regions.fg_path = @train/fg/join_path()
get_loud_regions.segment_size_in_seconds = 1.0
get_loud_regions.hop_size_in_seconds = .5
get_loud_regions.cache_location = %TRAIN_CACHE
get_loud_regions.overwrite = True
train/create_mix_coherent_closure.loud_regions = None

# Parameters for train/create_mix_incoherent_closure:
# ==============================================================================
train/create_mix_incoherent_closure.bg_event_parameters = \
    {'label': ('choose', []),
     'source_file': ('choose', []),
     'source_time': ('uniform', 0, 1000)}
train/create_mix_incoherent_closure.bg_path = @train/fg/join_path()
train/create_mix_incoherent_closure.bitdepth = 16
train/create_mix_incoherent_closure.fg_event_parameters = \
    {'event_duration': ('const', 12),
     'event_time': ('const', 0),
     'label': ('choose', ['s0', 's1']),
     'pitch_shift': ('uniform', -2, 2),
     'snr': ('uniform', -10, 0),
     'source_file': ('choose', []),
     'source_time': ('uniform', 0, 1000),
     'time_stretch': ('uniform', 0.8, 1.2)}
train/create_mix_incoherent_closure.num_sources = 2
train/create_mix_incoherent_closure.fg_path = @train/fg/join_path()
train/create_mix_incoherent_closure.ignore_warnings = True
train/create_mix_incoherent_closure.num_channels = 1
train/create_mix_incoherent_closure.quick_pitch_time = False
train/create_mix_incoherent_closure.ref_db = -40
train/create_mix_incoherent_closure.sample_rate = 16000
train/create_mix_incoherent_closure.scene_duration = 10

# Parameters for train/create_mix_incoherent_closure:
# ==============================================================================
train/create_coherent_incoherent_closure.balance = 0.2

# Parameters for DeepMaskEstimation:
# ==============================================================================
DeepMaskEstimation.model_path = @model_path()

# Parameters for evaluate:
# ==============================================================================
evaluate.block_on_gpu = True
evaluate.eval_class = @evaluator/unginify
evaluate.num_workers = 10
evaluate.output_folder = @output_folder()
evaluate.seed = 0
evaluate.separation_algorithm = @separator/unginify
evaluate.use_threadpool = True

# Parameters for separator/unginify:
# ==============================================================================
separator/unginify.kls = @nussl.separation.deep.DeepMaskEstimation
separator/unginify.kls_name = 'nussl.separation.deep.DeepMaskEstimation'

# Parameters for evaluator/unginify:
# ==============================================================================
evaluator/unginify.kls = @nussl.evaluation.BSSEvalScale
evaluator/unginify.kls_name = 'nussl.evaluation.BSSEvalScale'

# Parameters for test/join_path:
# ==============================================================================
test/join_path.base_path = %SCAPER_SOURCE_MATERIAL
test/join_path.relative_path = 'test'

# Parameters for MixSourceFolder:
# ==============================================================================
MixSourceFolder.stft_params = @stft_params/unginify()

# Parameters for test/MixSourceFolder:
# ==============================================================================
test/MixSourceFolder.folder = '/home/data/musdb/formatted/wav16k/test/'
test/MixSourceFolder.mix_folder = 'mixture'
test/MixSourceFolder.source_folders = ['vocals', 'accompaniment']

# Parameters for val/MixSourceFolder:
# ==============================================================================
val/MixSourceFolder.folder = '/home/data/musdb/formatted/wav16k/valid/'
val/MixSourceFolder.mix_folder = 'mixture'
val/MixSourceFolder.source_folders = ['vocals', 'accompaniment']
val/MixSourceFolder.transform = @val/build_transforms()
val/MixSourceFolder.num_channels = 1

# Parameters for model_path:
# ==============================================================================
model_path.model_suffix = 'checkpoints/latest.model.pth'

# Parameters for OnTheFly:
# ==============================================================================
OnTheFly.stft_params = @stft_params/unginify()
OnTheFly.num_channels = 1

# Parameters for train/OnTheFly:
# ==============================================================================
train/OnTheFly.mix_closure = @train/create_coherent_incoherent_closure()
train/OnTheFly.num_mixtures = 400000
train/OnTheFly.transform = @train/build_transforms()

# Parameters for ReduceLROnPlateau:
# ==============================================================================
ReduceLROnPlateau.factor = 0.5
ReduceLROnPlateau.patience = 100

# Parameters for STFTParams:
# ==============================================================================
STFTParams.hop_length = 128
STFTParams.window_length = 512
STFTParams.window_type = 'sqrt_hann'

# Parameters for sweep:
# ==============================================================================
sweep.parameters = {'add_autoclip_gradient_handler.clip_percentile': [10]}

# Parameters for train:
# ==============================================================================
train.batch_size = 20
train.device = 'cuda'
train.loss_dictionary = \
    {'MaskInferenceLoss': {'class': 'L1Loss', 'weight': 1},
     'WhitenedKMeansLoss': {'weight': 20000}}
train.num_data_workers = 15
train.num_epochs = 25
train.epoch_length = 800
train.output_folder = @output_folder()
train.seed = 0
train.val_batch_size = 1
train.val_loss_dictionary = {'MaskInferenceLoss': {'class': 'MSELoss', 'weight': 1}}

# Parameters for stft_params/unginify:
# ==============================================================================
stft_params/unginify.kls = @nussl.STFTParams
stft_params/unginify.kls_name = 'nussl.STFTParams'
